{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T13:42:20.761377Z",
     "iopub.status.busy": "2022-08-26T13:42:20.760877Z",
     "iopub.status.idle": "2022-08-26T13:42:31.493837Z",
     "shell.execute_reply": "2022-08-26T13:42:31.492810Z",
     "shell.execute_reply.started": "2022-08-26T13:42:20.761343Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint/ernie-3.0-base-zh-model\n",
      "/home/aistudio/project/user_data/model_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-26 21:42:25,639] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-base-zh'.\n",
      "[2022-08-26 21:42:25,642] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt\n",
      "[2022-08-26 21:42:25,672] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json\n",
      "[2022-08-26 21:42:25,676] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json\n",
      "[2022-08-26 21:42:26,513] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams\n",
      "W0826 21:42:26.518667 10209 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0826 21:42:26.523867 10209 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "import emojiswitch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import paddle.nn.functional as F\n",
    "from functools import partial\n",
    "from paddlenlp.data import Stack, Dict, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from paddlenlp.transformers.auto.tokenizer import AutoTokenizer\n",
    "from paddlenlp.transformers.auto.modeling import AutoModelForSequenceClassification\n",
    "\n",
    "seed = 10000\n",
    "def set_seed(seed):\n",
    "    paddle.seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(seed)\n",
    "# 超参数\n",
    "MODEL_NAME = 'ernie-3.0-base-zh'\n",
    "# 设置最大阶段长度 和 batch_size\n",
    "#max_seq_length = 200\n",
    "max_seq_length = 175\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "test_batch_size = 16\n",
    "# 训练过程中的最大学习率\n",
    "learning_rate = 2e-5\n",
    "# 训练轮次\n",
    "epochs = 4\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "#路径\n",
    "data_path = (os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir_curr = \"checkpoint/{}-model\".format(MODEL_NAME.replace('/','-'))\n",
    "print(save_dir_curr)\n",
    "save_dir_curr = os.path.join(data_path,\"user_data/model_data\")\n",
    "print(save_dir_curr)\n",
    "# 记录训练epoch、损失等值\n",
    "loggiing_print = 50\n",
    "loggiing_eval = 200\n",
    "# 提交文件名称\n",
    "#sumbit_name = \"work/sumbit.csv\"\n",
    "#model_logging_dir = 'work/model_logging.csv'\n",
    "# 是否开启 mutli-dropout\n",
    "enable_mdrop = True\n",
    "enable_adversarial = False\n",
    "layer_mode = 'dym' # cls / mean / max / dym\n",
    "\n",
    "train_path = os.path.join(data_path,\"xfdata/train.csv\")\n",
    "test_path = os.path.join(data_path,\"xfdata/train.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path,sep='\\t')[:1000]\n",
    "test = pd.read_csv(test_path,sep='\\t')[:100]\n",
    "import re\n",
    "def clean_str(text):\n",
    "    text = emojiswitch.demojize(text,delimiters=(\"\",\"\"), lang=\"zh\") # Emoji转文字\n",
    "    #text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \" \", text)# 去除正文中的@和回复/转发中的用户名\n",
    "    #text = re.sub(r\"\\[\\S+\\]\", \"\", text)      # 去除表情符号\n",
    "    URL_REGEX = re.compile(\n",
    "        r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "        re.IGNORECASE)\n",
    "    #text = re.sub(URL_REGEX, \"\", text)       # 去除网址\n",
    "    text = text.replace(\"转发微博\", \"\")       # 去除无意义的词语\n",
    "    text = text.replace('展开全文', '')\n",
    "    text = text.replace('?展开全文c', '')\n",
    "    text = text.replace('？', '?')\n",
    "    text = text.replace('！', '!')\n",
    "    text = text.replace('。', '.')\n",
    "    text = text.replace('，', ',')\n",
    "    text = text.replace('//?', '')\n",
    "    #?????????????????????????????????????????????????????????????????\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'?'\n",
    "        text = text.replace(word, '?')\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'.'\n",
    "        text = text.replace(word, '?')\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'!'\n",
    "        text = text.replace(word, '?')\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'@'\n",
    "        text = text.replace(word, '?')\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # 合并正文中过多的空格\n",
    "    return text.strip()\n",
    "train['text'] = train['text'].apply(lambda x: clean_str(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean_str(x))\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 创建数据迭代器iter\n",
    "def read(df,istrain=True):\n",
    "    if istrain:\n",
    "        for _,data in df.iterrows():\n",
    "            yield {\n",
    "                \"words\":data['text'],\n",
    "                \"labels\":data['label']\n",
    "                }\n",
    "    else:\n",
    "        for _,data in df.iterrows():\n",
    "            yield {\n",
    "                \"words\":data['text'],\n",
    "                }\n",
    "\n",
    "# 将生成器传入load_dataset\n",
    "train,valid = train_test_split(train,test_size=0.1,random_state=seed)\n",
    "train_ds = load_dataset(read, df=train, lazy=False)\n",
    "valid_ds = load_dataset(read, df=valid, lazy=False)\n",
    "# 编码\n",
    "def convert_example(example, tokenizer, max_seq_len=512, mode='train'):\n",
    "    # 调用tokenizer的数据处理方法把文本转为id\n",
    "    tokenized_input = tokenizer(example['words'],is_split_into_words=True,max_seq_len=max_seq_len)\n",
    "    if mode == \"test\":\n",
    "        return tokenized_input\n",
    "    # 把意图标签转为数字id\n",
    "    tokenized_input['labels'] = [example['labels']]\n",
    "    return tokenized_input # 字典形式，包含input_ids、token_type_ids、labels\n",
    "\n",
    "train_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='train',\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "valid_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='dev',\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "# 映射编码\n",
    "train_ds.map(train_trans_func, lazy=False)\n",
    "valid_ds.map(valid_trans_func, lazy=False)\n",
    "\n",
    "# 初始化BatchSampler\n",
    "np.random.seed(seed)\n",
    "train_batch_sampler = paddle.io.BatchSampler(train_ds, batch_size=train_batch_size, shuffle=True)\n",
    "valid_batch_sampler = paddle.io.BatchSampler(valid_ds, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "# 定义batchify_fn\n",
    "batchify_fn = lambda samples, fn = Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    \"labels\": Stack(dtype=\"int32\"),\n",
    "}): fn(samples)\n",
    "\n",
    "# 初始化DataLoader\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    collate_fn=batchify_fn,\n",
    "    return_list=True)\n",
    "valid_data_loader = paddle.io.DataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_sampler=valid_batch_sampler,\n",
    "    collate_fn=batchify_fn,\n",
    "    return_list=True)\n",
    "from paddlenlp.transformers.ernie.modeling import ErniePretrainedModel\n",
    "\n",
    "# 原始的基于Ernie的分类模型\n",
    "class EmotionErnieModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , _ = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "        sequence_output = sequence_output.mean(axis=1)\n",
    "        sequence_output = self.dropout(sequence_output) \n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n",
    "# 增加MultiDropout-Ernie的分类模型\n",
    "class Mdrop(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Mdrop,self).__init__()\n",
    "        self.dropout_0 = nn.Dropout(p=0)\n",
    "        self.dropout_1 = nn.Dropout(p=0.1)\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        self.dropout_3 = nn.Dropout(p=0.3)\n",
    "        self.dropout_4 = nn.Dropout(p=0.4)\n",
    "        self.dropout_5 = nn.Dropout(p=0.4)\n",
    "    def forward(self,x):\n",
    "        output_0 = self.dropout_0(x)\n",
    "        output_1 = self.dropout_1(x)\n",
    "        output_2 = self.dropout_2(x)\n",
    "        output_3 = self.dropout_3(x)\n",
    "        output_4 = self.dropout_4(x)\n",
    "        output_5 = self.dropout_5(x)\n",
    "        return [output_0,output_1,output_2,output_3,output_4,output_5]\n",
    "class EmotionMDropErnieModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        # 设置mutlidropout\n",
    "        self.dropout = Mdrop()\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , _ = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "        sequence_output = sequence_output.mean(axis=1)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        # 将mutlidropout进行pooling\n",
    "        sequence_output = paddle.mean(paddle.stack(sequence_output,axis=0),axis=0) \n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n",
    "\n",
    "# 不同嵌入策略的分类模型\n",
    "class EmotionLayerModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.dym_pool = nn.Linear(self.ernie.config['hidden_size'],1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def dym_pooling(self, avpooled_out, maxpooled_out):\n",
    "        pooled_output = [avpooled_out, maxpooled_out]\n",
    "        pool_logits = []\n",
    "        for i, layer in enumerate(pooled_output):\n",
    "            pool_logits.append(self.dym_pool(layer))\n",
    "        pool_logits = paddle.concat(pool_logits, axis=-1)\n",
    "        pool_dist = paddle.nn.functional.softmax(pool_logits)\n",
    "        pooled_out = paddle.concat([paddle.unsqueeze(x, 2) for x in pooled_output], axis=2)\n",
    "        pooled_out = paddle.unsqueeze(pooled_out, 1)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 2)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 1)\n",
    "        pooled_output = paddle.matmul(pooled_out, pool_dist)\n",
    "        pooled_output = paddle.squeeze(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , pooled_output = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "         # 选择嵌入策略\n",
    "        if layer_mode == \"mean\":\n",
    "            output = sequence_output.mean(axis=1)\n",
    "        elif layer_mode == \"max\":\n",
    "            output = sequence_output.max(axis=1)\n",
    "        elif layer_mode == \"dym\":\n",
    "            mean_output = sequence_output.mean(axis=1)\n",
    "            max_output = sequence_output.max(axis=1)\n",
    "            output = self.dym_pooling(mean_output,max_output)\n",
    "        else:\n",
    "            # 默认使用cls\n",
    "            output = pooled_output\n",
    "        output = self.dropout(output) \n",
    "        logits = self.classifier(output)\n",
    "        return logits \n",
    "# 改进后的模型\n",
    "class EmotionModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        if enable_mdrop:\n",
    "            self.dropout = Mdrop()\n",
    "        else:\n",
    "            self.dropout = nn.Dropout(self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.dym_pool = nn.Linear(self.ernie.config['hidden_size'],1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def dym_pooling(self, avpooled_out, maxpooled_out):\n",
    "        pooled_output = [avpooled_out, maxpooled_out]\n",
    "        pool_logits = []\n",
    "        for i, layer in enumerate(pooled_output):\n",
    "            pool_logits.append(self.dym_pool(layer))\n",
    "        pool_logits = paddle.concat(pool_logits, axis=-1)\n",
    "        pool_dist = paddle.nn.functional.softmax(pool_logits)\n",
    "        pooled_out = paddle.concat([paddle.unsqueeze(x, 2) for x in pooled_output], axis=2)\n",
    "        pooled_out = paddle.unsqueeze(pooled_out, 1)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 2)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 1)\n",
    "        pooled_output = paddle.matmul(pooled_out, pool_dist)\n",
    "        pooled_output = paddle.squeeze(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , pooled_output = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "        # 选择嵌入策略\n",
    "        if layer_mode == \"mean\":\n",
    "            output = sequence_output.mean(axis=1)\n",
    "        elif layer_mode == \"max\":\n",
    "            output = sequence_output.max(axis=1)\n",
    "        elif layer_mode == \"dym\":\n",
    "            mean_output = sequence_output.mean(axis=1)\n",
    "            max_output = sequence_output.max(axis=1)\n",
    "            output = self.dym_pooling(mean_output,max_output)\n",
    "        else:\n",
    "            # 默认使用cls\n",
    "            output = pooled_output\n",
    "        # 选择dropout\n",
    "        output = self.dropout(output)\n",
    "        if enable_mdrop:\n",
    "            output = paddle.mean(paddle.stack(output,axis=0),axis=0) \n",
    "        # 下游任务\n",
    "        logits = self.classifier(output)\n",
    "        return logits \n",
    "# 创建model\n",
    "label_classes = train['label'].unique()\n",
    "model = EmotionModel.from_pretrained(MODEL_NAME,num_classes=len(label_classes))\n",
    "# 训练总步数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# 学习率衰减策略\n",
    "lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps,warmup_proportion)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params,\n",
    "    grad_clip=paddle.nn.ClipGradByGlobalNorm(max_grad_norm))\n",
    "# utils - 对抗训练 FGM\n",
    "class FGM(object):\n",
    "    \"\"\"\n",
    "    Fast Gradient Method（FGM）\n",
    "    针对 embedding 层梯度上升干扰的对抗训练方法\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epsilon=1., emb_name='emb'):\n",
    "        # emb_name 这个参数要换成你模型中embedding 的参数名\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.emb_name = emb_name\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and self.emb_name in name:  # 检验参数是否可训练及范围\n",
    "                self.backup[name] = param.numpy()  # 备份原有参数值\n",
    "                grad_tensor = paddle.to_tensor(param.grad)  # param.grad 是个 numpy 对象\n",
    "                norm = paddle.norm(grad_tensor)  # norm 化\n",
    "                if norm != 0:\n",
    "                    r_at = self.epsilon * grad_tensor / norm\n",
    "                    param.add(r_at)  # 在原有 embed 值上添加向上梯度干扰\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and self.emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.set_value(self.backup[name])  # 将原有 embed 参数还原\n",
    "        self.backup = {}\n",
    "\n",
    "# 对抗训练\n",
    "if enable_adversarial:\n",
    "    adv = FGM(model=model,epsilon=1e-6,emb_name='word_embeddings')\n",
    "# 验证部分\n",
    "@paddle.no_grad()\n",
    "def evaluation(model, data_loader):\n",
    "    model.eval()\n",
    "    real_s = []\n",
    "    pred_s = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits,axis=1)\n",
    "        pred_s.extend(probs.argmax(axis=1).numpy())\n",
    "        real_s.extend(labels.reshape([-1]).numpy())\n",
    "    score =  accuracy_score(y_pred=pred_s,y_true=real_s)\n",
    "    return score\n",
    "\n",
    "# 训练阶段\n",
    "def do_train(model,data_loader):\n",
    "    total_loss = 0.\n",
    "    model_total_epochs = 0\n",
    "    best_score = 0.9\n",
    "    training_loss = 0\n",
    "    # 训练\n",
    "    print(\"train ...\")\n",
    "    train_time = time.time()\n",
    "    valid_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(0, epochs):\n",
    "        preds,reals = [],[]\n",
    "        for step, batch in enumerate(data_loader, start=1):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            logits = model(input_ids, token_type_ids)\n",
    "            loss = F.softmax_with_cross_entropy(logits,labels).mean()\n",
    "\n",
    "            probs = F.softmax(logits,axis=1)\n",
    "            preds.extend(probs.argmax(axis=1))\n",
    "            reals.extend(labels.reshape([-1]))\n",
    "            \n",
    "            loss.backward()\n",
    "            # 对抗训练\n",
    "            if enable_adversarial:\n",
    "                adv.attack()  # 在 embedding 上添加对抗扰动\n",
    "                adv_logits = model(input_ids, token_type_ids)\n",
    "                adv_loss = F.softmax_with_cross_entropy(adv_logits,labels).mean()\n",
    "                adv_loss.backward()  # 反向传播，并在正常的 grad 基础上，累加对抗训练的梯度\n",
    "                adv.restore()  # 恢复 embedding 参数\n",
    "\n",
    "            total_loss +=  loss.numpy()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        \n",
    "            model_total_epochs += 1\n",
    "            if model_total_epochs % loggiing_print == 0:\n",
    "                train_acc = accuracy_score(preds,reals)\n",
    "                print(\"step: %d / %d, train acc: %.5f training loss: %.5f speed %.1f s\" % (model_total_epochs, num_training_steps, train_acc, total_loss/model_total_epochs,(time.time() - train_time)))\n",
    "                train_time = time.time()\n",
    "            \n",
    "            if model_total_epochs % loggiing_eval == 0:\n",
    "                eval_score = evaluation(model, valid_data_loader)\n",
    "                print(\"validation speed %.2f s\" % (time.time() - valid_time))\n",
    "                valid_time = time.time()\n",
    "                if best_score  < eval_score:\n",
    "                    print(\"eval acc: %.5f acc update %.5f ---> %.5f \" % (eval_score,best_score,eval_score))\n",
    "                    best_score  = eval_score\n",
    "                    # 保存模型\n",
    "                    os.makedirs(save_dir_curr,exist_ok=True)\n",
    "                    save_param_path = os.path.join(save_dir_curr, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "                    # 保存tokenizer\n",
    "                    tokenizer.save_pretrained(save_dir_curr)\n",
    "                else:\n",
    "                    print(\"eval acc: %.5f but best acc %.5f \" % (eval_score,best_score))\n",
    "                model.train()\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T13:43:27.697521Z",
     "iopub.status.busy": "2022-08-26T13:43:27.696691Z",
     "iopub.status.idle": "2022-08-26T13:43:31.625342Z",
     "shell.execute_reply": "2022-08-26T13:43:31.624064Z",
     "shell.execute_reply.started": "2022-08-26T13:43:27.697480Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict start ...\n",
      "predict end ...\n"
     ]
    }
   ],
   "source": [
    "# 相同方式构造测试集\n",
    "test_ds = load_dataset(read,df=test, istrain=False, lazy=False)\n",
    "\n",
    "test_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='test',\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "test_ds.map(test_trans_func, lazy=False)\n",
    "\n",
    "test_batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "test_batchify_fn = lambda samples, fn = Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "}): fn(samples)\n",
    "\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=test_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "# 预测阶段\n",
    "def do_sample_predict(model,data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids= batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits,axis=1)\n",
    "        preds.extend(probs.argmax(axis=1).numpy())\n",
    "    return preds\n",
    "\n",
    "# 读取最佳模型\n",
    "state_dict = paddle.load(os.path.join(save_dir_curr,'model_best.pdparams'))\n",
    "model.load_dict(state_dict)\n",
    "\n",
    "# 预测\n",
    "print(\"predict start ...\")\n",
    "pred_score = do_sample_predict(model,test_data_loader)\n",
    "print(\"predict end ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T13:43:53.878248Z",
     "iopub.status.busy": "2022-08-26T13:43:53.877627Z",
     "iopub.status.idle": "2022-08-26T13:43:53.895235Z",
     "shell.execute_reply": "2022-08-26T13:43:53.894198Z",
     "shell.execute_reply.started": "2022-08-26T13:43:53.878209Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumbit = pd.DataFrame([],columns=['label'])\n",
    "sumbit[\"label\"] = pred_score\n",
    "sumbit_path = os.path.join(data_path,\"prediction_result/result.csv\")\n",
    "sumbit.to_csv(sumbit_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
