{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T13:10:21.733370Z",
     "iopub.status.busy": "2022-08-26T13:10:21.732868Z",
     "iopub.status.idle": "2022-08-26T13:10:27.396084Z",
     "shell.execute_reply": "2022-08-26T13:10:27.394937Z",
     "shell.execute_reply.started": "2022-08-26T13:10:21.733341Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.3.7)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.1.96)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (3.20.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.64.0)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.3.3)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.9.8)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (4.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (0.9.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (2.24.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.19.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (2022.7.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp) (3.0.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp) (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=2.0.0->paddlenlp) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: emojiswitch in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 将paddlenlp更新至最新版本\n",
    "!pip install -U paddlenlp\n",
    "# emoji转换成文字\n",
    "!pip install emojiswitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T13:10:27.399799Z",
     "iopub.status.busy": "2022-08-26T13:10:27.398857Z",
     "iopub.status.idle": "2022-08-26T13:12:11.569423Z",
     "shell.execute_reply": "2022-08-26T13:12:11.568430Z",
     "shell.execute_reply.started": "2022-08-26T13:10:27.399759Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint/ernie-3.0-base-zh-model\n",
      "/home/aistudio/project/user_data/model_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-26 21:11:13,430] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-base-zh'.\n",
      "[2022-08-26 21:11:13,433] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt\n",
      "[2022-08-26 21:11:13,460] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json\n",
      "[2022-08-26 21:11:13,463] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json\n",
      "[2022-08-26 21:12:08,567] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams\n",
      "W0826 21:12:08.571808  5188 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0826 21:12:08.580546  5188 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "import emojiswitch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import paddle.nn.functional as F\n",
    "from functools import partial\n",
    "from paddlenlp.data import Stack, Dict, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from paddlenlp.transformers.auto.tokenizer import AutoTokenizer\n",
    "from paddlenlp.transformers.auto.modeling import AutoModelForSequenceClassification\n",
    "\n",
    "seed = 10000\n",
    "def set_seed(seed):\n",
    "    paddle.seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(seed)\n",
    "# 超参数\n",
    "MODEL_NAME = 'ernie-3.0-base-zh'\n",
    "# 设置最大阶段长度 和 batch_size\n",
    "#max_seq_length = 200\n",
    "max_seq_length = 175\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "test_batch_size = 16\n",
    "# 训练过程中的最大学习率\n",
    "learning_rate = 2e-5\n",
    "# 训练轮次\n",
    "epochs = 4\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "#路径\n",
    "data_path = (os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir_curr = \"checkpoint/{}-model\".format(MODEL_NAME.replace('/','-'))\n",
    "print(save_dir_curr)\n",
    "save_dir_curr = os.path.join(data_path,\"user_data/model_data\")\n",
    "print(save_dir_curr)\n",
    "# 记录训练epoch、损失等值\n",
    "loggiing_print = 50\n",
    "loggiing_eval = 200\n",
    "# 提交文件名称\n",
    "#sumbit_name = \"work/sumbit.csv\"\n",
    "#model_logging_dir = 'work/model_logging.csv'\n",
    "# 是否开启 mutli-dropout\n",
    "enable_mdrop = True\n",
    "enable_adversarial = False\n",
    "layer_mode = 'dym' # cls / mean / max / dym\n",
    "\n",
    "train_path = os.path.join(data_path,\"xfdata/train.csv\")\n",
    "test_path = os.path.join(data_path,\"xfdata/train.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path,sep='\\t')[:1000]\n",
    "test = pd.read_csv(test_path,sep='\\t')[:100]\n",
    "import re\n",
    "def clean_str(text):\n",
    "    text = emojiswitch.demojize(text,delimiters=(\"\",\"\"), lang=\"zh\") # Emoji转文字\n",
    "    #text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \" \", text)# 去除正文中的@和回复/转发中的用户名\n",
    "    #text = re.sub(r\"\\[\\S+\\]\", \"\", text)      # 去除表情符号\n",
    "    URL_REGEX = re.compile(\n",
    "        r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "        re.IGNORECASE)\n",
    "    #text = re.sub(URL_REGEX, \"\", text)       # 去除网址\n",
    "    text = text.replace(\"转发微博\", \"\")       # 去除无意义的词语\n",
    "    text = text.replace('展开全文', '')\n",
    "    text = text.replace('?展开全文c', '')\n",
    "    text = text.replace('？', '?')\n",
    "    text = text.replace('！', '!')\n",
    "    text = text.replace('。', '.')\n",
    "    text = text.replace('，', ',')\n",
    "    text = text.replace('//?', '')\n",
    "    #?????????????????????????????????????????????????????????????????\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'?'\n",
    "        text = text.replace(word, '?')\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'.'\n",
    "        text = text.replace(word, '?')\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'!'\n",
    "        text = text.replace(word, '?')\n",
    "    for i in range(66, 1, -1):\n",
    "        word = i*'@'\n",
    "        text = text.replace(word, '?')\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # 合并正文中过多的空格\n",
    "    return text.strip()\n",
    "train['text'] = train['text'].apply(lambda x: clean_str(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean_str(x))\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 创建数据迭代器iter\n",
    "def read(df,istrain=True):\n",
    "    if istrain:\n",
    "        for _,data in df.iterrows():\n",
    "            yield {\n",
    "                \"words\":data['text'],\n",
    "                \"labels\":data['label']\n",
    "                }\n",
    "    else:\n",
    "        for _,data in df.iterrows():\n",
    "            yield {\n",
    "                \"words\":data['text'],\n",
    "                }\n",
    "\n",
    "# 将生成器传入load_dataset\n",
    "train,valid = train_test_split(train,test_size=0.1,random_state=seed)\n",
    "train_ds = load_dataset(read, df=train, lazy=False)\n",
    "valid_ds = load_dataset(read, df=valid, lazy=False)\n",
    "# 编码\n",
    "def convert_example(example, tokenizer, max_seq_len=512, mode='train'):\n",
    "    # 调用tokenizer的数据处理方法把文本转为id\n",
    "    tokenized_input = tokenizer(example['words'],is_split_into_words=True,max_seq_len=max_seq_len)\n",
    "    if mode == \"test\":\n",
    "        return tokenized_input\n",
    "    # 把意图标签转为数字id\n",
    "    tokenized_input['labels'] = [example['labels']]\n",
    "    return tokenized_input # 字典形式，包含input_ids、token_type_ids、labels\n",
    "\n",
    "train_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='train',\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "valid_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='dev',\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "# 映射编码\n",
    "train_ds.map(train_trans_func, lazy=False)\n",
    "valid_ds.map(valid_trans_func, lazy=False)\n",
    "\n",
    "# 初始化BatchSampler\n",
    "np.random.seed(seed)\n",
    "train_batch_sampler = paddle.io.BatchSampler(train_ds, batch_size=train_batch_size, shuffle=True)\n",
    "valid_batch_sampler = paddle.io.BatchSampler(valid_ds, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "# 定义batchify_fn\n",
    "batchify_fn = lambda samples, fn = Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    \"labels\": Stack(dtype=\"int32\"),\n",
    "}): fn(samples)\n",
    "\n",
    "# 初始化DataLoader\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    collate_fn=batchify_fn,\n",
    "    return_list=True)\n",
    "valid_data_loader = paddle.io.DataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_sampler=valid_batch_sampler,\n",
    "    collate_fn=batchify_fn,\n",
    "    return_list=True)\n",
    "from paddlenlp.transformers.ernie.modeling import ErniePretrainedModel\n",
    "\n",
    "# 原始的基于Ernie的分类模型\n",
    "class EmotionErnieModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , _ = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "        sequence_output = sequence_output.mean(axis=1)\n",
    "        sequence_output = self.dropout(sequence_output) \n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n",
    "# 增加MultiDropout-Ernie的分类模型\n",
    "class Mdrop(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Mdrop,self).__init__()\n",
    "        self.dropout_0 = nn.Dropout(p=0)\n",
    "        self.dropout_1 = nn.Dropout(p=0.1)\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        self.dropout_3 = nn.Dropout(p=0.3)\n",
    "        self.dropout_4 = nn.Dropout(p=0.4)\n",
    "        self.dropout_5 = nn.Dropout(p=0.4)\n",
    "    def forward(self,x):\n",
    "        output_0 = self.dropout_0(x)\n",
    "        output_1 = self.dropout_1(x)\n",
    "        output_2 = self.dropout_2(x)\n",
    "        output_3 = self.dropout_3(x)\n",
    "        output_4 = self.dropout_4(x)\n",
    "        output_5 = self.dropout_5(x)\n",
    "        return [output_0,output_1,output_2,output_3,output_4,output_5]\n",
    "class EmotionMDropErnieModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        # 设置mutlidropout\n",
    "        self.dropout = Mdrop()\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , _ = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "        sequence_output = sequence_output.mean(axis=1)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        # 将mutlidropout进行pooling\n",
    "        sequence_output = paddle.mean(paddle.stack(sequence_output,axis=0),axis=0) \n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n",
    "\n",
    "# 不同嵌入策略的分类模型\n",
    "class EmotionLayerModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.dym_pool = nn.Linear(self.ernie.config['hidden_size'],1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def dym_pooling(self, avpooled_out, maxpooled_out):\n",
    "        pooled_output = [avpooled_out, maxpooled_out]\n",
    "        pool_logits = []\n",
    "        for i, layer in enumerate(pooled_output):\n",
    "            pool_logits.append(self.dym_pool(layer))\n",
    "        pool_logits = paddle.concat(pool_logits, axis=-1)\n",
    "        pool_dist = paddle.nn.functional.softmax(pool_logits)\n",
    "        pooled_out = paddle.concat([paddle.unsqueeze(x, 2) for x in pooled_output], axis=2)\n",
    "        pooled_out = paddle.unsqueeze(pooled_out, 1)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 2)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 1)\n",
    "        pooled_output = paddle.matmul(pooled_out, pool_dist)\n",
    "        pooled_output = paddle.squeeze(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , pooled_output = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "         # 选择嵌入策略\n",
    "        if layer_mode == \"mean\":\n",
    "            output = sequence_output.mean(axis=1)\n",
    "        elif layer_mode == \"max\":\n",
    "            output = sequence_output.max(axis=1)\n",
    "        elif layer_mode == \"dym\":\n",
    "            mean_output = sequence_output.mean(axis=1)\n",
    "            max_output = sequence_output.max(axis=1)\n",
    "            output = self.dym_pooling(mean_output,max_output)\n",
    "        else:\n",
    "            # 默认使用cls\n",
    "            output = pooled_output\n",
    "        output = self.dropout(output) \n",
    "        logits = self.classifier(output)\n",
    "        return logits \n",
    "# 改进后的模型\n",
    "class EmotionModel(ErniePretrainedModel):\n",
    "    def __init__(self, ernie, num_classes=1, dropout=None):\n",
    "        super().__init__()\n",
    "        # 预训练模型\n",
    "        self.ernie = ernie\n",
    "        self.num_classes = num_classes\n",
    "        if enable_mdrop:\n",
    "            self.dropout = Mdrop()\n",
    "        else:\n",
    "            self.dropout = nn.Dropout(self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'],self.num_classes)\n",
    "        self.dym_pool = nn.Linear(self.ernie.config['hidden_size'],1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def dym_pooling(self, avpooled_out, maxpooled_out):\n",
    "        pooled_output = [avpooled_out, maxpooled_out]\n",
    "        pool_logits = []\n",
    "        for i, layer in enumerate(pooled_output):\n",
    "            pool_logits.append(self.dym_pool(layer))\n",
    "        pool_logits = paddle.concat(pool_logits, axis=-1)\n",
    "        pool_dist = paddle.nn.functional.softmax(pool_logits)\n",
    "        pooled_out = paddle.concat([paddle.unsqueeze(x, 2) for x in pooled_output], axis=2)\n",
    "        pooled_out = paddle.unsqueeze(pooled_out, 1)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 2)\n",
    "        pool_dist = paddle.unsqueeze(pool_dist, 1)\n",
    "        pooled_output = paddle.matmul(pooled_out, pool_dist)\n",
    "        pooled_output = paddle.squeeze(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        sequence_output , pooled_output = self.ernie(input_ids,token_type_ids=token_type_ids)\n",
    "        # 选择嵌入策略\n",
    "        if layer_mode == \"mean\":\n",
    "            output = sequence_output.mean(axis=1)\n",
    "        elif layer_mode == \"max\":\n",
    "            output = sequence_output.max(axis=1)\n",
    "        elif layer_mode == \"dym\":\n",
    "            mean_output = sequence_output.mean(axis=1)\n",
    "            max_output = sequence_output.max(axis=1)\n",
    "            output = self.dym_pooling(mean_output,max_output)\n",
    "        else:\n",
    "            # 默认使用cls\n",
    "            output = pooled_output\n",
    "        # 选择dropout\n",
    "        output = self.dropout(output)\n",
    "        if enable_mdrop:\n",
    "            output = paddle.mean(paddle.stack(output,axis=0),axis=0) \n",
    "        # 下游任务\n",
    "        logits = self.classifier(output)\n",
    "        return logits \n",
    "# 创建model\n",
    "label_classes = train['label'].unique()\n",
    "model = EmotionModel.from_pretrained(MODEL_NAME,num_classes=len(label_classes))\n",
    "# 训练总步数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# 学习率衰减策略\n",
    "lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps,warmup_proportion)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params,\n",
    "    grad_clip=paddle.nn.ClipGradByGlobalNorm(max_grad_norm))\n",
    "# utils - 对抗训练 FGM\n",
    "class FGM(object):\n",
    "    \"\"\"\n",
    "    Fast Gradient Method（FGM）\n",
    "    针对 embedding 层梯度上升干扰的对抗训练方法\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epsilon=1., emb_name='emb'):\n",
    "        # emb_name 这个参数要换成你模型中embedding 的参数名\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.emb_name = emb_name\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and self.emb_name in name:  # 检验参数是否可训练及范围\n",
    "                self.backup[name] = param.numpy()  # 备份原有参数值\n",
    "                grad_tensor = paddle.to_tensor(param.grad)  # param.grad 是个 numpy 对象\n",
    "                norm = paddle.norm(grad_tensor)  # norm 化\n",
    "                if norm != 0:\n",
    "                    r_at = self.epsilon * grad_tensor / norm\n",
    "                    param.add(r_at)  # 在原有 embed 值上添加向上梯度干扰\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and self.emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.set_value(self.backup[name])  # 将原有 embed 参数还原\n",
    "        self.backup = {}\n",
    "\n",
    "# 对抗训练\n",
    "if enable_adversarial:\n",
    "    adv = FGM(model=model,epsilon=1e-6,emb_name='word_embeddings')\n",
    "# 验证部分\n",
    "@paddle.no_grad()\n",
    "def evaluation(model, data_loader):\n",
    "    model.eval()\n",
    "    real_s = []\n",
    "    pred_s = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits,axis=1)\n",
    "        pred_s.extend(probs.argmax(axis=1).numpy())\n",
    "        real_s.extend(labels.reshape([-1]).numpy())\n",
    "    score =  accuracy_score(y_pred=pred_s,y_true=real_s)\n",
    "    return score\n",
    "\n",
    "# 训练阶段\n",
    "def do_train(model,data_loader):\n",
    "    total_loss = 0.\n",
    "    model_total_epochs = 0\n",
    "    best_score = 0.9\n",
    "    training_loss = 0\n",
    "    # 训练\n",
    "    print(\"train ...\")\n",
    "    train_time = time.time()\n",
    "    valid_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(0, epochs):\n",
    "        preds,reals = [],[]\n",
    "        for step, batch in enumerate(data_loader, start=1):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            logits = model(input_ids, token_type_ids)\n",
    "            loss = F.softmax_with_cross_entropy(logits,labels).mean()\n",
    "\n",
    "            probs = F.softmax(logits,axis=1)\n",
    "            preds.extend(probs.argmax(axis=1))\n",
    "            reals.extend(labels.reshape([-1]))\n",
    "            \n",
    "            loss.backward()\n",
    "            # 对抗训练\n",
    "            if enable_adversarial:\n",
    "                adv.attack()  # 在 embedding 上添加对抗扰动\n",
    "                adv_logits = model(input_ids, token_type_ids)\n",
    "                adv_loss = F.softmax_with_cross_entropy(adv_logits,labels).mean()\n",
    "                adv_loss.backward()  # 反向传播，并在正常的 grad 基础上，累加对抗训练的梯度\n",
    "                adv.restore()  # 恢复 embedding 参数\n",
    "\n",
    "            total_loss +=  loss.numpy()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        \n",
    "            model_total_epochs += 1\n",
    "            if model_total_epochs % loggiing_print == 0:\n",
    "                train_acc = accuracy_score(preds,reals)\n",
    "                print(\"step: %d / %d, train acc: %.5f training loss: %.5f speed %.1f s\" % (model_total_epochs, num_training_steps, train_acc, total_loss/model_total_epochs,(time.time() - train_time)))\n",
    "                train_time = time.time()\n",
    "            \n",
    "            if model_total_epochs % loggiing_eval == 0:\n",
    "                eval_score = evaluation(model, valid_data_loader)\n",
    "                print(\"validation speed %.2f s\" % (time.time() - valid_time))\n",
    "                valid_time = time.time()\n",
    "                if best_score  < eval_score:\n",
    "                    print(\"eval acc: %.5f acc update %.5f ---> %.5f \" % (eval_score,best_score,eval_score))\n",
    "                    best_score  = eval_score\n",
    "                    # 保存模型\n",
    "                    os.makedirs(save_dir_curr,exist_ok=True)\n",
    "                    save_param_path = os.path.join(save_dir_curr, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "                    # 保存tokenizer\n",
    "                    tokenizer.save_pretrained(save_dir_curr)\n",
    "                else:\n",
    "                    print(\"eval acc: %.5f but best acc %.5f \" % (eval_score,best_score))\n",
    "                model.train()\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T13:12:11.571639Z",
     "iopub.status.busy": "2022-08-26T13:12:11.570820Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ...\n",
      "step: 50 / 6752, train acc: 0.53687 training loss: 0.68983 speed 16.8 s\n",
      "step: 100 / 6752, train acc: 0.55937 training loss: 0.68166 speed 16.0 s\n",
      "step: 150 / 6752, train acc: 0.59333 training loss: 0.66571 speed 16.2 s\n",
      "step: 200 / 6752, train acc: 0.63687 training loss: 0.63590 speed 17.1 s\n",
      "validation speed 82.54 s\n",
      "eval acc: 0.84217 but best acc 0.90000 \n",
      "step: 250 / 6752, train acc: 0.68450 training loss: 0.57503 speed 34.6 s\n",
      "step: 300 / 6752, train acc: 0.72760 training loss: 0.50808 speed 18.4 s\n",
      "step: 350 / 6752, train acc: 0.75938 training loss: 0.45625 speed 19.2 s\n",
      "step: 400 / 6752, train acc: 0.78352 training loss: 0.41521 speed 20.0 s\n",
      "validation speed 92.57 s\n",
      "eval acc: 0.96133 acc update 0.90000 ---> 0.96133 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-26 21:15:07,947] [    INFO] - tokenizer config file saved in /home/aistudio/project/user_data/model_data/tokenizer_config.json\n",
      "[2022-08-26 21:15:07,950] [    INFO] - Special tokens file saved in /home/aistudio/project/user_data/model_data/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 450 / 6752, train acc: 0.80243 training loss: 0.38459 speed 38.4 s\n",
      "step: 500 / 6752, train acc: 0.81812 training loss: 0.35849 speed 20.9 s\n",
      "step: 550 / 6752, train acc: 0.83119 training loss: 0.33594 speed 24.4 s\n",
      "step: 600 / 6752, train acc: 0.84208 training loss: 0.31760 speed 22.6 s\n"
     ]
    }
   ],
   "source": [
    "best_score = do_train(model,train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"best pearsonr score: %.5f\" % best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logging part\n",
    "\n",
    "logging_dir = os.path.join(data_path, 'user_data/tmp_data')\n",
    "logging_name = os.path.join(logging_dir,'run_logging.csv')\n",
    "os.makedirs(logging_dir,exist_ok=True)\n",
    "\n",
    "var = [MODEL_NAME, seed, learning_rate, max_seq_length, layer_mode, enable_mdrop, enable_adversarial, best_score]\n",
    "names = ['model', 'seed', 'lr', \"max_len\" , 'layer_mode', 'enable_mdrop', 'enable_adversarial', 'best_score']\n",
    "vars_dict = {k: v for k, v in zip(names, var)}\n",
    "results = dict(**vars_dict)\n",
    "keys = list(results.keys())\n",
    "values = list(results.values())\n",
    "\n",
    "if not os.path.exists(logging_name):    \n",
    "    ori = []\n",
    "    ori.append(values)\n",
    "    logging_df = pd.DataFrame(ori, columns=keys)\n",
    "    logging_df.to_csv(logging_name, index=False)\n",
    "else:\n",
    "    logging_df= pd.read_csv(logging_name)\n",
    "    new = pd.DataFrame(results, index=[1])\n",
    "    logging_df = logging_df.append(new, ignore_index=True)\n",
    "    logging_df.to_csv(logging_name, index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 相同方式构造测试集\n",
    "test_ds = load_dataset(read,df=test, istrain=False, lazy=False)\n",
    "\n",
    "test_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='test',\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "test_ds.map(test_trans_func, lazy=False)\n",
    "\n",
    "test_batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "test_batchify_fn = lambda samples, fn = Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "}): fn(samples)\n",
    "\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=test_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "# 预测阶段\n",
    "def do_sample_predict(model,data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids= batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits,axis=1)\n",
    "        preds.extend(probs.argmax(axis=1).numpy())\n",
    "    return preds\n",
    "\n",
    "# 读取最佳模型\n",
    "state_dict = paddle.load(os.path.join(save_dir_curr,'model_best.pdparams'))\n",
    "model.load_dict(state_dict)\n",
    "\n",
    "# 预测\n",
    "print(\"predict start ...\")\n",
    "pred_score = do_sample_predict(model,test_data_loader)\n",
    "print(\"predict end ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 根据运行日志生成编号，对应日志编号的生成提交文件名称\n",
    "# 例如sumbit_emtion1.csv 就代表日志index为1的提交结果文件\n",
    "result_record = len(logging_df) - 1\n",
    "sumbit = pd.DataFrame([],columns=['label'])\n",
    "sumbit[\"label\"] = pred_score\n",
    "sumbit_path = os.path.join(data_path,\"prediction_result/result.csv\")\n",
    "sumbit.to_csv(sumbit_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
